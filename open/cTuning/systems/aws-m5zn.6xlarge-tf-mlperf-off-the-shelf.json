{
  "accelerator_frequency": "",
  "accelerator_host_interconnect": "",
  "accelerator_interconnect": "",
  "accelerator_interconnect_topology": "",
  "accelerator_memory_capacity": "N/A",
  "accelerator_memory_configuration": "",
  "accelerator_model_name": "N/A",
  "accelerator_on-chip_memories": "",
  "accelerators_per_node": "0",
  "cooling": "",
  "division": "open",
  "framework": "TensorFlow v2.6.0 (out-of-the-box MLPerf)",
  "host_memory_capacity": "384GB",
  "host_memory_configuration": "6 slots / 32GB each / 2934 MT/s per socket",
  "host_networking": "",
  "host_networking_topology": "",
  "host_processor_caches": "L1d cache: 384 KiB; L1i cache: 384 KiB; L2 cache: 12 MiB; L3 cache: 24.8 MiB",
  "host_processor_core_count": "12",
  "host_processor_frequency": "3800 MHz",
  "host_processor_interconnect": "",
  "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8252C CPU @ 3.80GHz",
  "host_processors_per_node": "1",
  "host_storage_capacity": "50 GiB",
  "host_storage_type": "SSD",
  "hw_notes": "https://aws.amazon.com/blogs/aws/new-ec2-m5zn-instances-fastest-intel-xeon-scalable-cpu-in-the-cloud ; vCPU: 24",
  "number_of_nodes": "1",
  "operating_system": "Ubuntu 20.04.2 LTS (Linux-5.8.0-1041-aws-x86_64-with-glibc2.29)",
  "other_software_stack": "5.4.0-45-generic; Python 3.8.10; GCC 9.3.0",
  "status": "available",
  "submitter": "cTuning",
  "sw_notes": "Automated by MLCommons Collective Knowledge v2.5.8 (https://github.com/mlcommons/ck) and the CK-powered MLPerf submission workflow (https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)",
  "system_name": "Amazon EC2 (m5zn.6xlarge)",
  "system_type": "datacenter"
}
