{
  "accelerator_frequency": "",
  "accelerator_host_interconnect": "",
  "accelerator_interconnect": "",
  "accelerator_interconnect_topology": "",
  "accelerator_memory_capacity": "N/A",
  "accelerator_memory_configuration": "",
  "accelerator_model_name": "N/A",
  "accelerator_on-chip_memories": "",
  "accelerators_per_node": "0",
  "cooling": "",
  "division": "open",
  "framework": "TensorFlow v2.6.0 (out-of-the-box MLPerf)",
  "host_memory_capacity": "320GB",
  "host_memory_configuration": "",
  "host_networking": "",
  "host_networking_topology": "",
  "host_processor_caches": "L1d cache: 1.3 MiB; L1i cache: 1.3 MiB; L2 cache: 40 MiB; L3 cache: 66 MiB",
  "host_processor_core_count": "20",
  "host_processor_frequency": "2800MHz",
  "host_processor_interconnect": "",
  "host_processor_model_name": "Intel(R) Xeon(R) CPU (Intel Cascade Lake CPU platform)",
  "host_processors_per_node": "2",
  "host_storage_capacity": "150 GiB",
  "host_storage_type": "Balanced",
  "hw_notes": "vCPU: 80",
  "number_of_nodes": "1",
  "operating_system": "Ubuntu 20.04.2 LTS (Linux-5.8.0-1038-gcp-x86_64-with-glibc2.29)",
  "other_software_stack": "5.4.0-45-generic; Python 3.8.10; GCC 9.3.0",
  "status": "available",
  "submitter": "cTuning",
  "sw_notes": "Automated by MLCommons Collective Knowledge v2.5.8 (https://github.com/mlcommons/ck) and the CK-powered MLPerf submission workflow (https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)",
  "system_name": "Google (Google Compute Engine) n2-standard-80",
  "system_type": "datacenter"
}
